---
title: "Text Mining in Python"
date: 2021-12-15 17:11:16
categories:
- python
- machineLeaning
tags:
- Study
- python
- machineLeaning
- TextMining
---

## ê°œìš”
- ë¹…ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” & í…ìŠ¤íŠ¸ ë§ˆì´ë‹
<br><br>



- Ref01_ [Matplotlib íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°](https://wikidocs.net/92112)
- Ref02_ [ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/94600)
- ë„¤ì´ë²„ ì‡¼í•‘ ë¦¬ë·° ê°ì„± ë¶„ë¥˜í•˜ê¸°(Naver Shopping Review Sentiment Analysis)


<br><br>

---

## í‰ê°€
- ë‹¤ìŒì€ ë„¤ì´ë²„ ì‡¼í•‘ ë¦¬ë·° ê°ì„± ë¶„ë¥˜í•˜ê¸° ì˜ˆì œì…ë‹ˆë‹¤. 
- ë¹ˆì¹¸ì— `# ì½”ë“œ ì…ë ¥`ë€ì— ì ë‹¹í•œ ì½”ë“œë¥¼ ì‘ì„±í•˜ì‹œê¸°ë¥¼ ë°”ëë‹ˆë‹¤. 
- ê° ë¹ˆì¹¸ë‹¹ 10ì ì…ë‹ˆë‹¤. 

### Colabì— Mecab ì„¤ì¹˜


```python
# Colabì— Mecab ì„¤ì¹˜
!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git
%cd Mecab-ko-for-Google-Colab
!bash install_mecab-ko_on_colab190912.sh
```

    Cloning into 'Mecab-ko-for-Google-Colab'...
    remote: Enumerating objects: 91, done.[K
    remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91[K
    Unpacking objects: 100% (91/91), done.
    /content/Mecab-ko-for-Google-Colab
    Installing konlpy.....
    Collecting konlpy
      Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.4 MB 2.4 MB/s 
    [?25hCollecting JPype1>=0.7.0
      Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448 kB 23.5 MB/s 
    [?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)
    Collecting colorama
      Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)
    Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)
    Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)
    Collecting beautifulsoup4==4.6.0
      Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86 kB 2.4 MB/s 
    [?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)
    Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)
    Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)
    Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)
    Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)
    Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)
    Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy
      Attempting uninstall: beautifulsoup4
        Found existing installation: beautifulsoup4 4.6.3
        Uninstalling beautifulsoup4-4.6.3:
          Successfully uninstalled beautifulsoup4-4.6.3
    Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2
    Done
    Installing mecab-0.996-ko-0.9.2.tar.gz.....
    Downloading mecab-0.996-ko-0.9.2.tar.gz.......
    from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz
    --2021-12-15 08:19:45--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz
    Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c0:3470, 2406:da00:ff00::22e9:9f55, ...
    Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Djk%2BX4VYfoZUGHzDRgTrcVVdFvE%3D&Expires=1639557778&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]
    --2021-12-15 08:19:46--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=Djk%2BX4VYfoZUGHzDRgTrcVVdFvE%3D&Expires=1639557778&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None
    Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.113.163
    Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.113.163|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 1414979 (1.3M) [application/x-tar]
    Saving to: â€˜mecab-0.996-ko-0.9.2.tar.gzâ€™
    
    mecab-0.996-ko-0.9. 100%[===================>]   1.35M  1.07MB/s    in 1.3s    
    
    2021-12-15 08:19:48 (1.07 MB/s) - â€˜mecab-0.996-ko-0.9.2.tar.gzâ€™ saved [1414979/1414979]
    
    Done
    Unpacking mecab-0.996-ko-0.9.2.tar.gz.......
    Done
    Change Directory to mecab-0.996-ko-0.9.2.......
    installing mecab-0.996-ko-0.9.2.tar.gz........
    configure
    make
    make check
    make install
    ldconfig
    Done
    Change Directory to /content
    Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......
    from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz
    --2021-12-15 08:21:19--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz
    Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::6b17:d1f5, 2406:da00:ff00::22cd:e0db, ...
    Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=ZNAR2x6%2FNWxJ4p%2BOkG%2BjdG77Dqk%3D&Expires=1639558279&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]
    --2021-12-15 08:21:19--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=ZNAR2x6%2FNWxJ4p%2BOkG%2BjdG77Dqk%3D&Expires=1639558279&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None
    Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 54.231.82.195
    Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|54.231.82.195|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 49775061 (47M) [application/x-tar]
    Saving to: â€˜mecab-ko-dic-2.1.1-20180720.tar.gzâ€™
    
    mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  13.0MB/s    in 4.5s    
    
    2021-12-15 08:21:25 (10.5 MB/s) - â€˜mecab-ko-dic-2.1.1-20180720.tar.gzâ€™ saved [49775061/49775061]
    
    Done
    Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......
    Done
    Change Directory to mecab-ko-dic-2.1.1-20180720
    Done
    installing........
    configure
    make
    make install
    apt-get update
    apt-get upgrade
    apt install curl
    apt install git
    bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)
    Done
    Successfully Installed
    Now you can use Mecab
    from konlpy.tag import Mecab
    mecab = Mecab()
    ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€ ë°©ë²• : https://bit.ly/3k0ZH53
    NameError: name 'Tagger' is not defined ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŸ°íƒ€ì„ì„ ì¬ì‹¤í–‰ í•´ì£¼ì„¸ìš”
    ë¸”ë¡œê·¸ì— í•´ê²° ë°©ë²•ì„ ë‚¨ê²¨ì£¼ì‹  tanaë‹˜ ê°ì‚¬í•©ë‹ˆë‹¤.
    

## ë„¤ì´ë²„ ì‡¼í•‘ ë¦¬ë·° ë°ì´í„°ì— ëŒ€í•œ ì´í•´ì™€ ì „ì²˜ë¦¬
- 


```python
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from collections import Counter
from konlpy.tag import Mecab
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

## ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°


```python
urllib.request.urlretrieve("https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt", filename="ratings_total.txt")
```




    ('ratings_total.txt', <http.client.HTTPMessage at 0x7f7d3557f750>)



- í•´ë‹¹ ë°ì´í„°ì—ëŠ” ì—´ ì œëª©ì´ ë³„ë„ë¡œ ì—†ìŒ. ê·¸ë˜ì„œ ì„ì˜ë¡œ ë‘ ê°œì˜ ì—´ì œëª©ì¸ "ratings"ì™€ "reviews" ì¶”ê°€


```python
# (1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê³ , ì „ì²´ ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥ # 200,000
totalDt = pd.read_table('ratings_total.txt', names=['ratings', 'reviews'])
print('ì „ì²´ ë¦¬ë·° ê°œìˆ˜ :',len(totalDt)) # ì „ì²´ ë¦¬ë·° ê°œìˆ˜ ì¶œë ¥
```

    ì „ì²´ ë¦¬ë·° ê°œìˆ˜ : 200000
    


```python
totalDt[:5]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ratings</th>
      <th>reviews</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5</td>
      <td>ë°°ê³µë¹ ë¥´ê³  êµ¿</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>íƒë°°ê°€ ì—‰ë§ì´ë„¤ìš© ì €í¬ì§‘ ë°‘ì—ì¸µì— ë§ë„ì—†ì´ ë†”ë‘ê³ ê°€ê³ </td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>ì•„ì£¼ì¢‹ì•„ìš” ë°”ì§€ ì •ë§ ì¢‹ì•„ì„œ2ê°œ ë” êµ¬ë§¤í–ˆì–´ìš” ì´ê°€ê²©ì— ëŒ€ë°•ì…ë‹ˆë‹¤. ë°”ëŠì§ˆì´ ì¡°ê¸ˆ ...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>ì„ ë¬¼ìš©ìœ¼ë¡œ ë¹¨ë¦¬ ë°›ì•„ì„œ ì „ë‹¬í–ˆì–´ì•¼ í•˜ëŠ” ìƒí’ˆì´ì—ˆëŠ”ë° ë¨¸ê·¸ì»µë§Œ ì™€ì„œ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤. ì „...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>ë¯¼íŠ¸ìƒ‰ìƒ ì˜ˆë»ìš”. ì˜† ì†ì¡ì´ëŠ” ê±°ëŠ” ìš©ë„ë¡œë„ ì‚¬ìš©ë˜ë„¤ìš” ã…ã…</td>
    </tr>
  </tbody>
</table>
</div>



- í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬í•˜ê¸°


```python
totalDt['label'] = np.select([totalDt.ratings > 3], [1], default=0)
totalDt[:5]
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ratings</th>
      <th>reviews</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5</td>
      <td>ë°°ê³µë¹ ë¥´ê³  êµ¿</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>íƒë°°ê°€ ì—‰ë§ì´ë„¤ìš© ì €í¬ì§‘ ë°‘ì—ì¸µì— ë§ë„ì—†ì´ ë†”ë‘ê³ ê°€ê³ </td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>ì•„ì£¼ì¢‹ì•„ìš” ë°”ì§€ ì •ë§ ì¢‹ì•„ì„œ2ê°œ ë” êµ¬ë§¤í–ˆì–´ìš” ì´ê°€ê²©ì— ëŒ€ë°•ì…ë‹ˆë‹¤. ë°”ëŠì§ˆì´ ì¡°ê¸ˆ ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>ì„ ë¬¼ìš©ìœ¼ë¡œ ë¹¨ë¦¬ ë°›ì•„ì„œ ì „ë‹¬í–ˆì–´ì•¼ í•˜ëŠ” ìƒí’ˆì´ì—ˆëŠ”ë° ë¨¸ê·¸ì»µë§Œ ì™€ì„œ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤. ì „...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>ë¯¼íŠ¸ìƒ‰ìƒ ì˜ˆë»ìš”. ì˜† ì†ì¡ì´ëŠ” ê±°ëŠ” ìš©ë„ë¡œë„ ì‚¬ìš©ë˜ë„¤ìš” ã…ã…</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



- ê° ì—´ì— ëŒ€í•´ì„œ ì¤‘ë³µì„ ì œì™¸í•œ ìƒ˜í”Œì˜ ìˆ˜ ì¹´ìš´íŠ¸


```python
totalDt['ratings'].nunique(), totalDt['reviews'].nunique(), totalDt['label'].nunique()
```




    (4, 199908, 2)



- ratingsì—´ì˜ ê²½ìš° 1, 2, 4, 5ë¼ëŠ” ë„¤ ê°€ì§€ ê°’ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. reviewsì—´ì—ì„œ ì¤‘ë³µì„ ì œì™¸í•œ ê²½ìš° 199,908ê°œì…ë‹ˆë‹¤. í˜„ì¬ 20ë§Œê°œì˜ ë¦¬ë·°ê°€ ì¡´ì¬í•˜ë¯€ë¡œ ì´ëŠ” í˜„ì¬ ê°–ê³  ìˆëŠ” ë°ì´í„°ì— ì¤‘ë³µì¸ ìƒ˜í”Œë“¤ì´ ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì¤‘ë³µì¸ ìƒ˜í”Œë“¤ì„ ì œê±°í•´ì¤ë‹ˆë‹¤.


```python
# (2) reviewì—´ì—ì„œ ì¤‘ë³µ ë°ì´í„° ì œê±° drop_duplicates() í•¨ìˆ˜ í™œìš©
totalDt.drop_duplicates(subset=['reviews'], inplace=True)
print('ì´ ìƒ˜í”Œì˜ ìˆ˜ :',len(totalDt))
```

    ì´ ìƒ˜í”Œì˜ ìˆ˜ : 199908
    

- NULL ê°’ ìœ ë¬´ í™•ì¸


```python
print(totalDt.isnull().values.any())
```

    False
    

- í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ 3:1 ë¹„ìœ¨ë¡œ ë¶„ë¦¬


```python
train_data, test_data = train_test_split(totalDt, test_size = 0.25, random_state = 42)
print('í›ˆë ¨ìš© ë¦¬ë·°ì˜ ê°œìˆ˜ :', len(train_data))
print('í…ŒìŠ¤íŠ¸ìš© ë¦¬ë·°ì˜ ê°œìˆ˜ :', len(test_data))
```

    í›ˆë ¨ìš© ë¦¬ë·°ì˜ ê°œìˆ˜ : 149931
    í…ŒìŠ¤íŠ¸ìš© ë¦¬ë·°ì˜ ê°œìˆ˜ : 49977
    

## ë ˆì´ë¸”ì˜ ë¶„í¬ í™•ì¸


```python
# (3) label 1, 0 ë§‰ëŒ€ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots(1,1,figsize=(7,5))
width = 0.15

plot_Dt= train_data['label'].value_counts().plot(kind = 'bar', color='orange', edgecolor='black').legend()

plt.title('train_data',fontsize=20) ## íƒ€ì´í‹€ ì¶œë ¥
plt.ylabel('Count',fontsize=10) ## yì¶• ë¼ë²¨ ì¶œë ¥
plt.show()

```


    
![train_data](../../imeges/python/output_22_0.png)
    



```python
print(train_data.groupby('label').size().reset_index(name = 'count'))
```

       label  count
    0      0  74918
    1      1  75013
    

- ë‘ ë ˆì´ë¸” ëª¨ë‘ ì•½ 7ë§Œ 5ì²œê°œë¡œ 50:50 ë¹„ìœ¨ì„ ê°€ì§

## ë°ì´í„° ì •ì œí•˜ê¸°
- ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ í•œê¸€ì„ ì œì™¸í•˜ê³  ëª¨ë‘ ì œê±°í•´ì¤ë‹ˆë‹¤. 


```python
# í•œê¸€ê³¼ ê³µë°±ì„ ì œì™¸í•˜ê³  ëª¨ë‘ ì œê±°
# (4) í•œê¸€ ë° ê³µë°± ì œì™¸í•œ ëª¨ë“  ê¸€ì ì œê±°
train_data['reviews'] = train_data['reviews'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
train_data['reviews'].replace('', np.nan, inplace=True)
print(train_data.isnull().sum())
```

    ratings    0
    reviews    0
    label      0
    dtype: int64
    

- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ê°™ì€ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤. 


```python
# (5) ë°ìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©í•˜ê¸°
# ì½”ë“œ 1 ì¤‘ë³µ ì œê±°
# ì½”ë“œ 2 ì •ê·œ í‘œí˜„ì‹ ìˆ˜í–‰
# ì½”ë“œ 3 ê³µë°±ì€ Null ê°’ìœ¼ë¡œ ë³€ê²½
# ì½”ë“œ 4 Null ê°’ ì œê±°
test_data.drop_duplicates(subset = ['reviews'], inplace=True) # ì¤‘ë³µ ì œê±°
test_data['reviews'] = test_data['reviews'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","") # ì •ê·œ í‘œí˜„ì‹ ìˆ˜í–‰
test_data['reviews'].replace('', np.nan, inplace=True) # ê³µë°±ì€ Null ê°’ìœ¼ë¡œ ë³€ê²½
test_data = test_data.dropna(how='any') # Null ê°’ ì œê±°
print('ì „ì²˜ë¦¬ í›„ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œì˜ ê°œìˆ˜ :',len(test_data))
```

    ì „ì²˜ë¦¬ í›„ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œì˜ ê°œìˆ˜ : 49977
    

## í† í°í™” 
- í˜•íƒœì†Œ ë¶„ì„ê¸° Mecabì„ ì‚¬ìš©í•˜ì—¬ í† í°í™” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤. 


```python
# (6) Mecab í´ë˜ìŠ¤ í˜¸ì¶œí•˜ê¸°
mecab = Mecab()
print(mecab.morphs('ì™€ ì´ëŸ° ê²ƒë„ ìƒí’ˆì´ë¼ê³  ì°¨ë¼ë¦¬ ë‚´ê°€ ë§Œë“œëŠ” ê²Œ ë‚˜ì„ ë»”'))
```

    ['ì™€', 'ì´ëŸ°', 'ê²ƒ', 'ë„', 'ìƒí’ˆ', 'ì´', 'ë¼ê³ ', 'ì°¨ë¼ë¦¬', 'ë‚´', 'ê°€', 'ë§Œë“œ', 'ëŠ”', 'ê²Œ', 'ë‚˜ì„', 'ë»”']
    

- ë¶ˆìš©ì–´ë¥¼ ì§€ì •í•˜ì—¬ í•„ìš”ì—†ëŠ” í† í°ë“¤ì„ ì œê±°í•˜ë„ë¡ í•œë‹¤. 


```python
# (7) ë¶ˆìš©ì–´ ë§Œë“¤ê¸°
stopwords = ['ë„', 'ëŠ”', 'ë‹¤', 'ì˜', 'ê°€', 'ì´', 'ì€', 'í•œ', 'ì—', 'í•˜', 'ê³ ', 'ì„', 'ë¥¼', 'ì¸', 'ë“¯', 'ê³¼', 'ì™€', 'ë„¤', 'ë“¤', 'ë“¯', 'ì§€', 'ì„', 'ê²Œ']
```

- í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œ ë™ì¼í•œ ê³¼ì •ì„ ê±°ì¹œë‹¤. 


```python
train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)
train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])
```


```python
test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)
test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])
```

## ë‹¨ì–´ì™€ ê¸¸ì´ ë¶„í¬ í™•ì¸í•˜ê¸°
ê¸ì • ë¦¬ë·°ì—ëŠ” ì£¼ë¡œ ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ë§ì´ ë“±ì¥í•˜ê³ , ë¶€ì • ë¦¬ë·°ì—ëŠ” ì£¼ë¡œ ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ë“±ì¥í•˜ëŠ”ì§€ ë‘ ê°€ì§€ ê²½ìš°ì— ëŒ€í•´ì„œ ê° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê³„ì‚°í•´ë³´ê² ìŠµë‹ˆë‹¤. ê° ë ˆì´ë¸”ì— ë”°ë¼ì„œ ë³„ë„ë¡œ ë‹¨ì–´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥í•´ì¤ë‹ˆë‹¤.




```python
negative_W = np.hstack(train_data[train_data.label == 0]['tokenized'].values)
positive_W = np.hstack(train_data[train_data.label == 1]['tokenized'].values)
negative_W
positive_W
```




    array(['ì ë‹¹', 'ë§Œì¡±', 'í•©ë‹ˆë‹¤', ..., 'ì˜', 'ì‚¿', 'ì–´ìš”'], dtype='<U25')



- Counter()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë‹¨ì–´ì— ëŒ€í•œ ë¹ˆë„ìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•œë‹¤. ìš°ì„  ë¶€ì • ë¦¬ë·°ì— ëŒ€í•´ì„œ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 20ê°œ ë‹¨ì–´ ì¶œë ¥


```python
negative_word_count = Counter(negative_W)
print(negative_word_count.most_common(20))
```

    [('ë„¤ìš”', 31799), ('ëŠ”ë°', 20295), ('ì•ˆ', 19718), ('ì–´ìš”', 14849), ('ìˆ', 13200), ('ë„ˆë¬´', 13058), ('í–ˆ', 11783), ('ì¢‹', 9812), ('ë°°ì†¡', 9677), ('ê°™', 8997), ('êµ¬ë§¤', 8876), ('ì–´', 8869), ('ê±°', 8854), ('ì—†', 8670), ('ì•„ìš”', 8642), ('ìŠµë‹ˆë‹¤', 8436), ('ê·¸ëƒ¥', 8355), ('ë˜', 8345), ('ì˜', 8029), ('ì•Š', 7984)]
    

'ë„¤ìš”', 'ëŠ”ë°', 'ì•ˆ', 'ì•Š', 'ë„ˆë¬´', 'ì—†' ë“±ê³¼ ê°™ì€ ë‹¨ì–´ë“¤ì´ ë¶€ì • ë¦¬ë·°ì—ì„œ ì£¼ë¡œ ë“±ì¥í•©ë‹ˆë‹¤. ê¸ì • ë¦¬ë·°ì— ëŒ€í•´ì„œë„ ë™ì¼í•˜ê²Œ ì¶œë ¥í•´ë´…ì‹œë‹¤.


```python
positive_word_count = Counter(positive_W)
print(positive_word_count.most_common(20))
```

    [('ì¢‹', 39488), ('ì•„ìš”', 21184), ('ë„¤ìš”', 19895), ('ì–´ìš”', 18686), ('ì˜', 18602), ('êµ¬ë§¤', 16171), ('ìŠµë‹ˆë‹¤', 13320), ('ìˆ', 12391), ('ë°°ì†¡', 12275), ('ëŠ”ë°', 11670), ('í–ˆ', 9818), ('í•©ë‹ˆë‹¤', 9801), ('ë¨¹', 9635), ('ì¬', 9273), ('ë„ˆë¬´', 8397), ('ê°™', 7868), ('ë§Œì¡±', 7261), ('ê±°', 6482), ('ì–´', 6294), ('ì“°', 6292)]
    

'ì¢‹', 'ì•„ìš”', 'ë„¤ìš”', 'ì˜', 'ë„ˆë¬´', 'ë§Œì¡±' ë“±ê³¼ ê°™ì€ ë‹¨ì–´ë“¤ì´ ì£¼ë¡œ ë§ì´ ë“±ì¥í•©ë‹ˆë‹¤. ë‘ ê°€ì§€ ê²½ìš°ì— ëŒ€í•´ì„œ ê°ê° ê¸¸ì´ ë¶„í¬ë¥¼ í™•ì¸í•´ë´…ì‹œë‹¤.


```python
# (8) ê¸ì • ë¦¬ë·°ì™€ ë¶€ì • ë¦¬ë·° íˆìŠ¤í† ê·¸ë¨ ì‘ì„±í•˜ê¸°

fig,(ax1,ax2) = plt.subplots(1,2,figsize=(9,5))
text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))
ax1.hist(text_len, color='pink', edgecolor='black')
ax1.set_title('Positive Reviews')
ax1.set_xlabel('length of samples')
ax1.set_ylabel('number of samples')
print('ê¸ì • ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :', np.mean(text_len))

text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))
ax2.hist(text_len, color='skyblue', edgecolor='black')
ax2.set_title('ë¶€ì • ë¦¬ë·°')
ax2.set_title('Negative Reviews')
fig.suptitle('Words in texts')
ax2.set_xlabel('length of samples')
ax2.set_ylabel('number of samples')
print('ë¶€ì • ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :', np.mean(text_len))
plt.show()

```

    ê¸ì • ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ : 13.5877381253916
    ë¶€ì • ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ : 17.02948557089084
    


    
![Review_Histogram](../../imeges/python/output_43_1.png)
    


- ê¸ì • ë¦¬ë·°ë³´ë‹¤ëŠ” ë¶€ì • ë¦¬ë·°ê°€ ì¢€ ë” ê¸¸ê²Œ ì‘ì„±ëœ ê²½í–¥ì´ ìˆëŠ” ê²ƒ ê°™ë‹¤. 


```python
X_train = train_data['tokenized'].values
y_train = train_data['label'].values
X_test= test_data['tokenized'].values
y_test = test_data['label'].values
```

## ì •ìˆ˜ ì¸ì½”ë”©
- ì´ì œ ê¸°ê³„ê°€ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì •ìˆ˜ ì¸ì½”ë”©ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ìš°ì„ , í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ì„œ ë‹¨ì–´ ì§‘í•©(vocaburary)ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤.


```python
# (9) ì •ìˆ˜ ì¸ì½”ë”© í´ë˜ìŠ¤ í˜¸ì¶œ ë° X_train ë°ì´í„°ì— ì í•©í•˜ê¸°
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
```

ë‹¨ì–´ ì§‘í•©ì´ ìƒì„±ë˜ëŠ” ë™ì‹œì— ê° ë‹¨ì–´ì— ê³ ìœ í•œ ì •ìˆ˜ê°€ ë¶€ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” tokenizer.word_indexë¥¼ ì¶œë ¥í•˜ì—¬ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë“±ì¥ íšŸìˆ˜ê°€ 1íšŒì¸ ë‹¨ì–´ë“¤ì€ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ë°°ì œí•˜ê³ ì í•©ë‹ˆë‹¤. ì´ ë‹¨ì–´ë“¤ì´ ì´ ë°ì´í„°ì—ì„œ ì–¼ë§Œí¼ì˜ ë¹„ì¤‘ì„ ì°¨ì§€í•˜ëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.


```python
threshold = 2
total_cnt = len(tokenizer.word_index) # ë‹¨ì–´ì˜ ìˆ˜
rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸
total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©
rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)
print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))
print("ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:", (rare_cnt / total_cnt)*100)
print("ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:", (rare_freq / total_freq)*100)
```

    ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° : 39998
    ë“±ì¥ ë¹ˆë„ê°€ 1ë²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: 18213
    ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨: 45.53477673883694
    ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨: 0.7935698749320282
    

ë‹¨ì–´ê°€ ì•½ 40,000ê°œê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë“±ì¥ ë¹ˆë„ê°€ threshold ê°’ì¸ 2íšŒ ë¯¸ë§Œ. ì¦‰, 1íšŒì¸ ë‹¨ì–´ë“¤ì€ ë‹¨ì–´ ì§‘í•©ì—ì„œ ì•½ 45%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œë¡œ í›ˆë ¨ ë°ì´í„°ì—ì„œ ë“±ì¥ ë¹ˆë„ë¡œ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì€ ë§¤ìš° ì ì€ ìˆ˜ì¹˜ì¸ ì•½ 0.8%ë°–ì— ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì•„ë¬´ë˜ë„ ë“±ì¥ ë¹ˆë„ê°€ 1íšŒì¸ ë‹¨ì–´ë“¤ì€ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ë³„ë¡œ ì¤‘ìš”í•˜ì§€ ì•Šì„ ë“¯ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ ë‹¨ì–´ë“¤ì€ ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ë°°ì œì‹œí‚¤ê² ìŠµë‹ˆë‹¤.

ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ 1ì¸ ë‹¨ì–´ë“¤ì˜ ìˆ˜ë¥¼ ì œì™¸í•œ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ë‹¨ì–´ ì§‘í•©ì˜ ìµœëŒ€ í¬ê¸°ë¡œ ì œí•œí•˜ê² ìŠµë‹ˆë‹¤.


```python
# ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ ì¤‘ ë¹ˆë„ìˆ˜ 2ì´í•˜ì¸ ë‹¨ì–´ ê°œìˆ˜ëŠ” ì œê±°.
# 0ë²ˆ íŒ¨ë”© í† í°ê³¼ 1ë²ˆ OOV í† í°ì„ ê³ ë ¤í•˜ì—¬ +2
vocab_size = total_cnt - rare_cnt + 2
print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° :',vocab_size)
```

    ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : 21787
    

ì´ì œ ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ëŠ” 21,787ê°œì…ë‹ˆë‹¤. ì´ë¥¼ í† í¬ë‚˜ì´ì €ì˜ ì¸ìë¡œ ë„˜ê²¨ì£¼ë©´, í† í¬ë‚˜ì´ì €ëŠ” í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ìˆ«ì ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì´ë³´ë‹¤ í° ìˆ«ìê°€ ë¶€ì—¬ëœ ë‹¨ì–´ë“¤ì€ OOVë¡œ ë³€í™˜í•˜ê² ìŠµë‹ˆë‹¤.


```python
# (10) í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤ í˜¸ì¶œ ë° OOV ë³€í™˜ ì½”ë“œ ì‘ì„±
# ì½”ë“œ 1
# ì½”ë“œ 2

tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') 
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
```

ì •ìˆ˜ ì¸ì½”ë”©ì´ ì§„í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ ì X_trainê³¼ X_testì— ëŒ€í•´ì„œ ìƒìœ„ 3ê°œì˜ ìƒ˜í”Œë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.


```python
print(X_train[:3])
```

    [[67, 2060, 299, 14259, 263, 73, 6, 236, 168, 137, 805, 2951, 625, 2, 77, 62, 207, 40, 1343, 155, 3, 6], [482, 409, 52, 8530, 2561, 2517, 339, 2918, 250, 2357, 38, 473, 2], [46, 24, 825, 105, 35, 2372, 160, 7, 10, 8061, 4, 1319, 29, 140, 322, 41, 59, 160, 140, 7, 1916, 2, 113, 162, 1379, 323, 119, 136]]
    


```python
print(X_test[:3])
```

    [[14, 704, 767, 116, 186, 252, 12], [339, 3904, 62, 3816, 1651], [11, 69, 2, 49, 164, 3, 27, 15, 6, 1, 513, 289, 17, 92, 110, 564, 59, 7, 2]]
    

## íŒ¨ë”©
ì´ì œ ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ì˜ ìƒ˜í”Œë“¤ì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ëŠ” íŒ¨ë”© ì‘ì—…ì„ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ì—ì„œ ê°€ì¥ ê¸¸ì´ê°€ ê¸´ ë¦¬ë·°ì™€ ì „ì²´ ë°ì´í„°ì˜ ê¸¸ì´ ë¶„í¬ë¥¼ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.


```python
print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in X_train))
print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, X_train))/len(X_train))
plt.hist([len(s) for s in X_train], bins=35, label='bins=35', color="skyblue")
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()
```

    ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ : 85
    ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ : 15.307521459871541

    
![LengthOfReview](../../imeges/python/output_58_1.png)


ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ëŠ” 85, í‰ê·  ê¸¸ì´ëŠ” ì•½ 15ì…ë‹ˆë‹¤.

ê·¸ë¦¬ê³  ê·¸ë˜í”„ë¡œ ë´¤ì„ ë•Œ, ì „ì²´ì ìœ¼ë¡œëŠ” 60ì´í•˜ì˜ ê¸¸ì´ë¥¼ ê°€ì§€ëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.


```python
def below_threshold_len(max_len, nested_list):
  count = 0
  for sentence in nested_list:
    if(len(sentence) <= max_len):
        count = count + 1
  print('ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s'%(max_len, (count / len(nested_list))*100))

  
```

- ìµœëŒ€ ê¸¸ì´ê°€ 85ì´ë¯€ë¡œ ë§Œì•½ 80ìœ¼ë¡œ íŒ¨ë”©í•  ê²½ìš°, ëª‡ ê°œì˜ ìƒ˜í”Œë“¤ì„ ì˜¨ì „íˆ ë³´ì „í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.


```python
max_len = 80
below_threshold_len(max_len, X_train)
```

    ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ 80 ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: 99.99933302652553
    

í›ˆë ¨ìš© ë¦¬ë·°ì˜ 99.99%ê°€ 80ì´í•˜ì˜ ê¸¸ì´ë¥¼ ê°€ì§‘ë‹ˆë‹¤. í›ˆë ¨ìš© ë¦¬ë·°ë¥¼ ê¸¸ì´ 80ìœ¼ë¡œ íŒ¨ë”©í•˜ê² ìŠµë‹ˆë‹¤.


```python
X_train = pad_sequences(X_train, maxlen = max_len)
X_test = pad_sequences(X_test, maxlen = max_len)
```

# GRUë¡œ ë„¤ì´ë²„ ì‡¼í•‘ ë¦¬ë·° ê°ì„± ë¶„ë¥˜í•˜ê¸°


```python
from tensorflow.keras.layers import Embedding, Dense, GRU
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

embedding_dim = 100
hidden_units = 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim))
model.add(GRU(hidden_units))
model.add(Dense(1, activation='sigmoid'))

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)

def sentiment_predict(new_sentence):
  new_sentence = re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£ ]','', new_sentence)
  new_sentence = mecab.morphs(new_sentence) # í† í°í™”
  new_sentence = [word for word in new_sentence if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°
  encoded = tokenizer.texts_to_sequences([new_sentence]) # ì •ìˆ˜ ì¸ì½”ë”©
  pad_new = pad_sequences(encoded, maxlen = max_len) # íŒ¨ë”©

  score = float(model.predict(pad_new)) # ì˜ˆì¸¡
  if(score > 0.5):
    print("{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format(score * 100))
  else:
    print("{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.".format((1 - score) * 100))
```

    Epoch 1/15
    1875/1875 [==============================] - ETA: 0s - loss: 0.2725 - acc: 0.8967
    Epoch 00001: val_acc improved from -inf to 0.91916, saving model to best_model.h5
    1875/1875 [==============================] - 54s 25ms/step - loss: 0.2725 - acc: 0.8967 - val_loss: 0.2301 - val_acc: 0.9192
    Epoch 2/15
    1875/1875 [==============================] - ETA: 0s - loss: 0.2158 - acc: 0.9213
    Epoch 00002: val_acc improved from 0.91916 to 0.92240, saving model to best_model.h5
    1875/1875 [==============================] - 43s 23ms/step - loss: 0.2158 - acc: 0.9213 - val_loss: 0.2137 - val_acc: 0.9224
    Epoch 3/15
    1875/1875 [==============================] - ETA: 0s - loss: 0.1985 - acc: 0.9289
    Epoch 00003: val_acc improved from 0.92240 to 0.92637, saving model to best_model.h5
    1875/1875 [==============================] - 44s 24ms/step - loss: 0.1985 - acc: 0.9289 - val_loss: 0.2060 - val_acc: 0.9264
    Epoch 4/15
    1873/1875 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9332
    Epoch 00004: val_acc did not improve from 0.92637
    1875/1875 [==============================] - 43s 23ms/step - loss: 0.1878 - acc: 0.9332 - val_loss: 0.2031 - val_acc: 0.9260
    Epoch 5/15
    1874/1875 [============================>.] - ETA: 0s - loss: 0.1783 - acc: 0.9369
    Epoch 00005: val_acc improved from 0.92637 to 0.92670, saving model to best_model.h5
    1875/1875 [==============================] - 46s 24ms/step - loss: 0.1783 - acc: 0.9369 - val_loss: 0.2030 - val_acc: 0.9267
    Epoch 6/15
    1873/1875 [============================>.] - ETA: 0s - loss: 0.1698 - acc: 0.9405
    Epoch 00006: val_acc improved from 0.92670 to 0.92764, saving model to best_model.h5
    1875/1875 [==============================] - 44s 24ms/step - loss: 0.1697 - acc: 0.9405 - val_loss: 0.2055 - val_acc: 0.9276
    Epoch 7/15
    1873/1875 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9436
    Epoch 00007: val_acc did not improve from 0.92764
    1875/1875 [==============================] - 44s 24ms/step - loss: 0.1610 - acc: 0.9437 - val_loss: 0.2098 - val_acc: 0.9244
    Epoch 8/15
    1875/1875 [==============================] - ETA: 0s - loss: 0.1526 - acc: 0.9473
    Epoch 00008: val_acc did not improve from 0.92764
    1875/1875 [==============================] - 44s 23ms/step - loss: 0.1526 - acc: 0.9473 - val_loss: 0.2269 - val_acc: 0.9189
    Epoch 9/15
    1875/1875 [==============================] - ETA: 0s - loss: 0.1435 - acc: 0.9507
    Epoch 00009: val_acc did not improve from 0.92764
    1875/1875 [==============================] - 44s 24ms/step - loss: 0.1435 - acc: 0.9507 - val_loss: 0.2258 - val_acc: 0.9204
    Epoch 00009: early stopping
    


```python
sentiment_predict('ì´ ìƒí’ˆ ì§„ì§œ ì‹«ì–´ìš”... êµí™˜í•´ì£¼ì„¸ìš”')
```

    99.03% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.
    


```python
sentiment_predict('ì´ ìƒí’ˆ ì§„ì§œ ì¢‹ì•„ì—¬... ê°•ì¶”í•©ë‹ˆë‹¤. ')
```

    99.51% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.
    
